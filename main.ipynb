{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules \n",
    "import numpy as np  \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils \n",
    "\n",
    "text = utils.load_data('data/simpsons/moes_tavern_lines.txt')[81:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_words, int2vocab, vocab2int = utils.preprocess_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_words, batch_size, seq_length):\n",
    "    '''\n",
    "    <argument>\n",
    "    int_words  : array of tokenized word with shape of 1 \n",
    "    batch_size : batch_size\n",
    "    seq_length : number of time steps \n",
    "    </argument>\n",
    "    \n",
    "    :return: batches with matrix of (n_batches, 2, batch_size, sequence_length)\n",
    "    '''\n",
    "    word_per_batch = batch_size*seq_length\n",
    "    n_batches = len(int_words) // word_per_batch\n",
    "    #Keep enough words to train \n",
    "    int_words = np.array(int_words[:word_per_batch*n_batches])\n",
    "    xdata = np.array(int_words).reshape(batch_size, -1)\n",
    "    ydata = np.roll(xdata, -1).reshape(batch_size, -1) #shift the data by one \n",
    "    x_batches = np.split(xdata, n_batches, axis=1)\n",
    "    y_batches = np.split(ydata, n_batches, axis=1)\n",
    "    return np.array(list(zip(x_batches, y_batches)))\n",
    "\n",
    "#debuging purpose test\n",
    "get_batches(np.arange(20), 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, etc)\n",
    "    \"\"\"\n",
    "    inputs = tf.placeholder(tf.int32, shape=[batch_size, seq_length], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=[batch_size, seq_length], name='targets')\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    return tf.contrib.layers.embed_sequence(input_data, vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(batch_size, lstm_size, dropout):\n",
    "    \"\"\"\n",
    "    <argument>\n",
    "    batch_size : initial_state's batch_size\n",
    "    lstm_size  : list of lstm_size, n_lstm layer = len(lstm_size) with its layer's size corresponding to 'lstm_size' item idx\n",
    "    </argument>\n",
    "    \n",
    "    :return: tuple (cells ,initial_state)\n",
    "    \"\"\"\n",
    "    def generate_lstm(num_layer):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_layer)\n",
    "        #lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=dropout)\n",
    "        return lstm \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([generate_lstm(size) for size in lstm_size])\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    #initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    return cells, initial_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "def build_loss(logits, targets, input_data_shape):\n",
    "    loss = seq2seq.sequence_loss(logits, targets, tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, lr, grad_clip):\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -grad_clip, grad_clip), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    return train_op "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, lr=1e-2,batch_size=128,seq_length=20,\n",
    "                 embed_dim=500, lstm_size=[620,620], dropout=0.2, grad_clip=1.0, sampling=False):\n",
    "        if sampling == True:\n",
    "            batch_size, seq_length = 1,1 \n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        self.inputs, self.targets = generate_inputs(batch_size, seq_length)\n",
    "        self.input_data_shape = tf.shape(self.inputs)\n",
    "        self.embed = get_embed(self.inputs, vocab_size, embed_dim)\n",
    "        self.cell, self.initial_state = build_rnn(batch_size, lstm_size=lstm_size,dropout=dropout)\n",
    "        self.outputs, self.final_state = tf.nn.dynamic_rnn(self.cell, self.embed,\n",
    "                                                           initial_state=self.initial_state, dtype=tf.float32)\n",
    "        self.logits = tf.contrib.layers.fully_connected(self.outputs, vocab_size, activation_fn=None)\n",
    "        self.probs = tf.nn.softmax(self.logits, name='probs')\n",
    "        self.loss = build_loss(self.logits, self.targets, self.input_data_shape)\n",
    "        self.train_op = build_optimizer(self.loss, lr, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><font size=5>Hyperparameters</i> \n",
    "\n",
    "<ul>\n",
    "    <li>batch_size - Number of sequences running through the network in one pass.</li>\n",
    "    <li>sequence_length - Number of words in the sequence the network is trained on. Larger is better typically, the network will learn </li>\n",
    "    <li>vocab_size - Total word in vocabulary </li>\n",
    "    <li>dropout - The dropout keep probability when training. If you're network is overfitting, try decreasing this.</li>\n",
    "    <li>grad_clip - grading clipping threshold</li>\n",
    "    <li>epoch - How many times you want to train the network</li>\n",
    "    <li>learning_rate - How fast you want to train the network, the lower the slower the model to reach the minima</li>\n",
    "    <li>embedding dimension - Size of word embedding </li>\n",
    "    <li>lstm_size - List of n_nodes of lstm, n_layers == len(lstm_size)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 20\n",
    "vocab_size = len(int2vocab)\n",
    "dropout = 0.8 \n",
    "grad_clip = 1. \n",
    "epoch = 100\n",
    "learning_rate = 1e-3\n",
    "embed_dim= 500\n",
    "lstm_size = [620,620]\n",
    "epoch = 120\n",
    "batches = get_batches(int_words,batch_size,sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vocab_size=vocab_size,batch_size=batch_size, \n",
    "            seq_length=sequence_length, embed_dim=embed_dim, grad_clip=grad_clip,\n",
    "            dropout=dropout,lstm_size=lstm_size)\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/checkpoint.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter = 0 \n",
    "    for e in range(epoch):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for x,y in batches:\n",
    "            feed = {model.inputs : x, \n",
    "                    model.targets : y,\n",
    "                    model.initial_state: new_state}\n",
    "            loss,final_state, _ = sess.run([model.loss, model.final_state, model.train_op], feed)\n",
    "            new_state = final_state\n",
    "        counter += 1\n",
    "        print(\"epoch {} : {:.4f}\".format(counter, loss))\n",
    "    saver.save(sess, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size,n=5):\n",
    "    \"\"\"\n",
    "    randomly pick word from probabilty distribution across predictions\n",
    "    <argument>\n",
    "    preds      : predictions with shape of (1,1, vocab_size)\n",
    "    vocab_size : n_classes\n",
    "    n          : pick most n probable words, if n is None or zero, argmax will be used instead of choosing from \n",
    "                 probabilty distribution.\n",
    "    </argument>\n",
    "    :return: one selected word from probabilty distrubution \n",
    "    \"\"\"\n",
    "    \n",
    "    # Squeeze preds with shape of (1,1,n_classes) into (n_classes)\n",
    "    p = np.squeeze(preds)\n",
    "    if n == None or n == 0:\n",
    "        picked = np.argmax(p)\n",
    "    elif n > 0:\n",
    "        p[np.argsort(p)[:-n]] = 0\n",
    "        p = p/np.sum(p)\n",
    "        picked = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "        \n",
    "    return picked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefining hyperparameters\n",
    "\n",
    "batch_size = 128\n",
    "sequence_length = 20\n",
    "vocab_size = len(int2vocab)\n",
    "dropout = 0.8 \n",
    "grad_clip = 1. \n",
    "epoch = 100\n",
    "learning_rate = 1e-3\n",
    "embed_dim= 500\n",
    "lstm_size = [620,620]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(checkpoint, n_samples, prime='the'):\n",
    "    \"\"\"\n",
    "    <argument>\n",
    "    checkpoints   : path to checkpoint directory\n",
    "    n_samples     : how many word will be generated\n",
    "    prime         : starting word that will be feeded into lstm and used to generate other n_samples word. If None \n",
    "                    prime word will be random word in vocabulary \n",
    "    \n",
    "    </argument>\n",
    "    :return:  Generated text with length of n_samples+1 text with prime word as the first word in the text \n",
    "    \"\"\"\n",
    "    \n",
    "    model = RNN(vocab_size=vocab_size,batch_size=batch_size, \n",
    "            seq_length=sequence_length, embed_dim=embed_dim, grad_clip=grad_clip,\n",
    "            dropout=dropout,lstm_size=lstm_size, sampling=True)\n",
    "    samples = []\n",
    "    samples.append(prime)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        x = np.zeros((1,1))\n",
    "        \n",
    "        \n",
    "        x[0][0] = vocab2int[prime] \n",
    "        feed = {model.inputs : x,\n",
    "               model.initial_state : new_state}\n",
    "        preds, new_state = sess.run([model.probs, model.final_state], feed)\n",
    "        p_ = pick_top_n(preds, vocab_size)\n",
    "        samples.append(int2vocab[p_])\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            feed = {model.inputs : x, model.initial_state : new_state}\n",
    "            preds, new_state = sess.run([model.probs, model.final_state], feed)\n",
    "            p_ = pick_top_n(preds, vocab_size)\n",
    "            x[0][0] = p_\n",
    "            samples.append(int2vocab[p_])\n",
    "            \n",
    "            \n",
    "    generated_text = ' '.join(samples)\n",
    "    punctuation = utils.punctuation_lookup()\n",
    "    \n",
    "    # Revert back tokenized punctuation to untokenized puncutation\n",
    "    for key, value in punctuation.items():\n",
    "        generated_text = generated_text.replace(value,'{}'.format(key))\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "print(sampling(checkpoint, 1000, prime='hello'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
